{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"joey_demo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Y4VofGiF2MMr"},"source":["Install the right PyTorch version for Joey NMT. Might have to restart the colab after installing Joey NMT."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bG-XRA4tX2oV","executionInfo":{"status":"ok","timestamp":1627003543831,"user_tz":-420,"elapsed":132776,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"ad55c6cb-3e01-4ca3-c02b-ded631ccdc1a"},"source":["!pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.8.0+cu101\n","  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n","\u001b[K     |████████████████████████████████| 763.5 MB 15 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n","Successfully installed torch-1.8.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"a4u741jiY2-O","executionInfo":{"status":"ok","timestamp":1627003572706,"user_tz":-420,"elapsed":21729,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"dd336278-9220-41fc-b144-e7dc80a12ce6"},"source":["!pip install joeynmt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting joeynmt\n","  Downloading joeynmt-1.3-py3-none-any.whl (84 kB)\n","\u001b[?25l\r\u001b[K     |███▉                            | 10 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 20 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 30 kB 34.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 40 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 51 kB 38.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 61 kB 41.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 71 kB 34.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 81 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 84 kB 3.7 MB/s \n","\u001b[?25hCollecting subword-nmt\n","  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n","Collecting sacrebleu>=1.3.6\n","  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n","\u001b[?25l\r\u001b[K     |██████                          | 10 kB 40.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 55.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40 kB 59.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51 kB 62.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n","\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.11.1)\n","Collecting torchtext==0.9.0\n","  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n","\u001b[K     |████████████████████████████████| 7.1 MB 26.2 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 68.2 MB/s \n","\u001b[?25hCollecting pylint\n","  Downloading pylint-2.9.5-py3-none-any.whl (375 kB)\n","\u001b[K     |████████████████████████████████| 375 kB 66.7 MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (1.8.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.16.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt) (7.1.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (57.2.0)\n","Collecting wrapt==1.11.1\n","  Downloading wrapt-1.11.1.tar.gz (27 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt) (3.2.2)\n","Collecting numpy==1.20.1\n","  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n","\u001b[K     |████████████████████████████████| 15.3 MB 104 kB/s \n","\u001b[?25hCollecting six==1.12\n","  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (2.5.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt) (3.7.4.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt) (4.41.1)\n","Collecting portalocker==2.0.0\n","  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.8.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.34.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.12.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.36.2)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.17.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.32.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.4.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.3.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt) (4.6.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (0.10.0)\n","Collecting isort<6,>=4.2.5\n","  Downloading isort-5.9.2-py3-none-any.whl (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 63.7 MB/s \n","\u001b[?25hCollecting astroid<2.7,>=2.6.5\n","  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n","\u001b[K     |████████████████████████████████| 231 kB 72.8 MB/s \n","\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (0.10.2)\n","Collecting mccabe<0.7,>=0.6\n","  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n","Collecting lazy-object-proxy>=1.4.0\n","  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 4.2 MB/s \n","\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n","  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n","\u001b[K     |████████████████████████████████| 743 kB 48.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.4.1)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.1.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt) (2018.9)\n","Building wheels for collected packages: wrapt\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68432 sha256=96533a97c8c3a2dd58a1a2eff3cb5b712bc64590f9024f808a3f8b454104bc88\n","  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n","Successfully built wrapt\n","Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n","  Attempting uninstall: six\n","    Found existing installation: six 1.15.0\n","    Uninstalling six-1.15.0:\n","      Successfully uninstalled six-1.15.0\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.12.1\n","    Uninstalling wrapt-1.12.1:\n","      Successfully uninstalled wrapt-1.12.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.10.0\n","    Uninstalling torchtext-0.10.0:\n","      Successfully uninstalled torchtext-0.10.0\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n","tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n","tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n","tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n","google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n","google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n","google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed astroid-2.6.5 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.5 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","six"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"dF1998yPmtbS"},"source":["## Download"]},{"cell_type":"code","metadata":{"id":"Z6cux99eZ-gW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003618192,"user_tz":-420,"elapsed":29049,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"e31b67f6-b60a-4b34-f64f-7736aef77964"},"source":["# Đọc dữ liệu từ google drive và set đường dẫn \n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","path ='/content/gdrive/My Drive/Machine_Translation/'\n","\n","vi_train = path + \"indomain-news/train.vi\"\n","vi_dev = path + \"indomain-news/dev.vi\"\n","vi_test = path + \"indomain-news/tst.vi\"\n","\n","en_train = path + \"indomain-news/train.en\"\n","en_dev = path + \"indomain-news/dev.en\"\n","en_test = path + \"indomain-news/tst.en\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R_kVvMcBbgLg","executionInfo":{"status":"ok","timestamp":1627003633974,"user_tz":-420,"elapsed":6027,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["# Tiền xử lý\n","import string,re\n","def preprocess_nmt(text):\n","    \"\"\"Preprocess the English-Vietnamese dataset.\"\"\"\n","    def no_space(char, prev_char):\n","        return char in string.punctuation and prev_char != ' '\n","\n","    # Replace non-breaking space with space, and convert uppercase letters to\n","    # lowercase ones\n","    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n","    # Insert space between words and punctuation marks\n","    text = re.sub(\"\\\\s+\", \" \", text)\n","    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n","           for i, char in enumerate(text)]\n","    return ''.join(out).strip()\n","import pandas as pd\n","def read_data_pandas(en_path, vi_path):\n","    source = []\n","    target = []\n","    skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n","    with open(en_path, \"r\", encoding=\"utf8\") as file:\n","        content = file.read().split(\"\\n\")\n","        for line in content:\n","          if line.strip() != \"\":    \n","              source.append(line.strip())      \n","\n","    with open(vi_path, \"r\", encoding=\"utf8\") as file:\n","        content = file.read().split(\"\\n\")\n","        for line in content:\n","          if line.strip() != \"\":\n","              target.append(line.strip())     \n","        \n","    df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n","\n","    df[\"source_sentence\"] = df[\"source_sentence\"].apply(lambda x : preprocess_nmt(x))\n","    df[\"target_sentence\"] = df[\"target_sentence\"].apply(lambda x : preprocess_nmt(x))\n","    return df\n","\n","df_train = read_data_pandas(en_train,vi_train)\n","df_dev = read_data_pandas(en_dev,vi_dev)\n","df_test = read_data_pandas(en_test,vi_test)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"J74gHLAgMS2t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003636948,"user_tz":-420,"elapsed":362,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"09f5b5cf-710a-480f-cd30-68b5adfc462d"},"source":["print(len(df_train),len(df_dev),len(df_test))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["20000 1007 1220\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WeIGVD2rcfFD","executionInfo":{"status":"ok","timestamp":1627003641498,"user_tz":-420,"elapsed":2147,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["# This section does the split between train/dev for the parallel corpora then saves them as separate files\n","# We use 1000 dev test and the given test set.\n","import csv\n","source_language = \"en\"\n","target_language = \"vi\"\n","with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n","  for index, row in df_train.iterrows():\n","    src_file.write(row[\"source_sentence\"]+\"\\n\")\n","    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n","    \n","with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n","  for index, row in df_dev.iterrows():\n","    src_file.write(row[\"source_sentence\"]+\"\\n\")\n","    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n","\n","with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as trg_file:\n","  for index, row in df_test.iterrows():\n","    src_file.write(row[\"source_sentence\"]+\"\\n\")\n","    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"zH-WvRI_PC5G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003645605,"user_tz":-420,"elapsed":388,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"71f72d70-23d6-4c4a-d366-b9363b4c611d"},"source":["!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["dev.en\tdev.vi\tgdrive\tsample_data  test.en  test.vi  train.en  train.vi\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LBVyCK9aiPcE"},"source":["We'll only use a subset of dev and test data."]},{"cell_type":"code","metadata":{"id":"yHm_0HlIeshK","executionInfo":{"status":"ok","timestamp":1627003650068,"user_tz":-420,"elapsed":974,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["import os\n","isFile = os.path.isdir(\"data/en-vi/\")\n","if isFile == False:\n","    os.makedirs(\"data/en-vi/\")\n","\n","!mv 'train.en' 'data/en-vi/train.en'\n","!mv 'dev.en' 'data/en-vi/dev.en'\n","!mv 'test.en' 'data/en-vi/test.en'\n","\n","!mv 'train.vi' 'data/en-vi/train.vi'\n","!mv 'dev.vi' 'data/en-vi/dev.vi'\n","!mv 'test.vi' 'data/en-vi/test.vi'\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FoveftbNCyv8"},"source":["The data is sentence-aligned, that means that source and target file contain one sentence per line which correspond to each other."]},{"cell_type":"code","metadata":{"id":"FLNgutcOcHK2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003654098,"user_tz":-420,"elapsed":469,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"8d32fe03-4085-482f-9a0f-f0240131595e"},"source":["! head data/en-vi/dev.en\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["13 cases consisted of one individual each from thailand , china , morocco and india , two individuals each from saudi arabia , ethiopia , iran and three individuals from the uae were detected through early reporting by individuals .\n","therefore , koreans have fermented vegetables as food reserves for the winter .\n","last season , an incensed fan phoned nottinghamshire police reporting nani 's red card in the champions league match against real madrid as a 'crime ' .\n","later , as a result of the pandemic in italy , on march 8 , 2020 the italian government ordered all cinemas to be closed , for up to a month .\n","one purpose and guiding principle of the precede–proceed model is to direct initial attention to outcomes , rather than inputs .\n","since 1996 , the net worth of people under 35 has dropped by more than 34 percent .\n","the best example of echolocation is found in bats .\n","\"in a statement to the journal nature biotechnology in february 2020 , us national institutes of health viral ecology unit chief vincent munster said , \" \"the general genomic layout and the general replication kinetics and the biology of the mers , sars and [sars -cov -2 ] viruses are very similar , so testing drugs which target relatively generic parts of these coronaviruses is a logical step \" \" . \"\n","vip korean drama starring jang na -ra and lee sang -yoon\n","instead , they allowed the establishment of a joint -stock company in 2010 to carry out the project following requests from the ministry of industry and trade , units belonging to who had been leasing the land .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BrP5Ujk0cXxA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003665257,"user_tz":-420,"elapsed":526,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"a4b053ad-d52c-4bb3-876b-84fb99f6cea5"},"source":["! head data/en-vi/dev.vi"],"execution_count":10,"outputs":[{"output_type":"stream","text":["13 trường hợp bao gồm một cá nhân mỗi người từ thái lan , trung quốc , morocco và ấn độ , hai cá nhân từ ả rập saudi , ethiopia , iran và ba cá nhân từ uae đã được phát hiện thông qua báo cáo sớm của các cá nhân .\n","do đó , người hàn quốc đã muối rau trái làm thức ăn dự trữ cho mùa đông .\n","\"mùa trước , một cổ động viên giận dữ gọi điện đến cảnh sát nottinghamshire thông báo rằng chiếc thẻ đỏ của nani trong trận thuộc giải champions league với real madrid là một \" \"tội ác \" \" . \"\n","sau đó , do hậu quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý đã ra lệnh đóng cửa tất cả các rạp chiếu phim , trong vòng một tháng .\n","một mục đích và nguyên tắc chỉ đạo của mô hình precede - proceed là để hướng sự chú ý ban đầu đến kết quả , chứ không phải là yếu tố đầu vào .\n","kể từ năm 1996 , khối tài sản ròng của những người dưới 35 tuổi đã giảm xuống hơn 34 % .\n","ví dụ tốt nhất của định vị bằng tiếng vang là ở loài dơi .\n","\"trong một tuyên bố với tạp chí công nghệ sinh học tự nhiên vào tháng 2 năm 2020 , giám đốc bộ phận sinh thái virus của viện sức khỏe quốc gia hoa kỳ vincent munster nói : \" \"bố cục gene nói chung và động lực nhân rộng chung và sinh học của mers , sars và [sars -cov -2 ] virus rất giống nhau , vì vậy các loại thuốc thử nghiệm nhắm vào các phần tương đối chung của các coronavirus này là một bước đi hợp lý \" \" . \"\n","bộ phim truyền hình vip hàn quốc với sự tham gia của jang na -ra và lee sang -yoon\n","thay vào đó , họ cho phép thành lập một công ty cổ phần vào năm 2010 để thực hiện dự án theo yêu cầu của bộ công thương , các đơn vị đã thuê đất .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ydKB8OS1c0r1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003669717,"user_tz":-420,"elapsed":379,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"91196d7d-af48-4857-8813-4c83e8549db5"},"source":["! wc -l data/en-vi/*"],"execution_count":11,"outputs":[{"output_type":"stream","text":["   1007 data/en-vi/dev.en\n","   1007 data/en-vi/dev.vi\n","   1220 data/en-vi/test.en\n","   1220 data/en-vi/test.vi\n","  20000 data/en-vi/train.en\n","  20000 data/en-vi/train.vi\n","  44454 total\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WZg4k1rem1jI"},"source":["## Subword model training"]},{"cell_type":"markdown","metadata":{"id":"vJl9sQq22c5Z"},"source":["We will use the `subword_nmt` library to split words into subwords (BPE) according to their frequency in the training corpus."]},{"cell_type":"code","metadata":{"id":"tGDmK7rnqc6r","executionInfo":{"status":"ok","timestamp":1627003674052,"user_tz":-420,"elapsed":365,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["import os"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wNsekzFd1BD","executionInfo":{"status":"ok","timestamp":1627003676521,"user_tz":-420,"elapsed":372,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["src_lang = 'en'\n","trg_lang = 'vi'\n","bpe_size = 4000\n","datadir = '/content/data/en-vi/'\n","name = f'{src_lang}_{trg_lang}_bpe{bpe_size}'\n","\n","\n","train_src_file = os.path.join(datadir, f'train.{src_lang}')\n","train_trg_file = os.path.join(datadir, f'train.{trg_lang}')\n","train_joint_file = os.path.join(datadir, f'train.{src_lang}-{trg_lang}')\n","dev_src_file = os.path.join(datadir, f'dev.{src_lang}')\n","dev_trg_file = os.path.join(datadir, f'dev.{trg_lang}')\n","test_src_file = os.path.join(datadir, f'test.{src_lang}')\n","test_trg_file = os.path.join(datadir, f'test.{trg_lang}')\n","src_files = {'train': train_src_file, 'dev': dev_src_file, 'test': test_src_file}\n","trg_files = {'train': train_trg_file, 'dev': dev_trg_file, 'test': test_trg_file}\n","\n","\n","vocab_src_file = os.path.join(datadir, f'vocab.{bpe_size}.{src_lang}')\n","vocab_trg_file = os.path.join(datadir, f'vocab.{bpe_size}.{trg_lang}')\n","bpe_file = os.path.join(datadir, f'bpe.codes.{bpe_size}')"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJPV_l_G2ny1"},"source":["Train a BPE model with 4000 symbols for both languages jointly."]},{"cell_type":"code","metadata":{"id":"eVYh30Mjm3zu","executionInfo":{"status":"ok","timestamp":1627003687376,"user_tz":-420,"elapsed":7433,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["! cat $train_src_file $train_trg_file > $train_joint_file\n","\n","! subword-nmt learn-bpe \\\n","  --input $train_joint_file \\\n","  -s $bpe_size \\\n","  -o $bpe_file"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QEQjXFqv2u-3"},"source":["This file contains the merges of character sequences that subwords are made of."]},{"cell_type":"code","metadata":{"id":"ZtUq1rg1sA8H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003692107,"user_tz":-420,"elapsed":360,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"89d10304-9bdb-41a2-93d7-2747867d4ac6"},"source":["! head $bpe_file"],"execution_count":15,"outputs":[{"output_type":"stream","text":["#version: 0.2\n","n g</w>\n","t h\n","t r\n","a n\n","c h\n","th e</w>\n","t i\n","i n\n","n h\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tWdUshWa2z3V"},"source":["We apply the learned BPE merges to training, development and test data.\n"]},{"cell_type":"code","metadata":{"id":"Qgdj1d3wpA1c","executionInfo":{"status":"ok","timestamp":1627003701544,"user_tz":-420,"elapsed":4902,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["src_bpe_files = {}\n","trg_bpe_files = {}\n","for split in ['train', 'dev', 'test']:\n","  src_input_file = src_files[split]\n","  trg_input_file = trg_files[split]\n","  src_output_file = src_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n","  trg_output_file = trg_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n","  src_bpe_files[split] = src_output_file\n","  trg_bpe_files[split] = trg_output_file\n","\n","  ! subword-nmt apply-bpe \\\n","    -c $bpe_file \\\n","    < $src_input_file > $src_output_file\n","\n","  ! subword-nmt apply-bpe \\\n","    -c $bpe_file \\\n","    < $trg_input_file > $trg_output_file\n"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2KrKE20C27z3"},"source":["The subword-split data contains `@@ ` to indicate where words were split into subwords."]},{"cell_type":"code","metadata":{"id":"SZvrbM5Qx418","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003705945,"user_tz":-420,"elapsed":372,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"d056a36e-bdff-4c0d-82bd-8d3c1a1589f8"},"source":["! head data/en-vi/dev.4000.bpe.en"],"execution_count":17,"outputs":[{"output_type":"stream","text":["13 cases con@@ s@@ is@@ ted of one individu@@ al each from thailand , china , mor@@ oc@@ co and india , two individuals each from sau@@ di ara@@ bia , e@@ thi@@ op@@ ia , iran and three individuals from the u@@ a@@ e were det@@ ected through early repor@@ ting by individuals .\n","the@@ re@@ fore , kore@@ ans have fer@@ men@@ ted ve@@ get@@ ab@@ les as food re@@ ser@@ ves for the win@@ ter .\n","last seas@@ on , an inc@@ en@@ sed f@@ an ph@@ oned no@@ t@@ ti@@ ng@@ ha@@ m@@ shi@@ re police repor@@ ting n@@ an@@ i 's red car@@ d in the ch@@ amp@@ i@@ ons lea@@ gue mat@@ ch against real mad@@ ri@@ d as a '@@ cri@@ me ' .\n","later , as a result of the pandemic in italy , on march 8 , 2020 the it@@ alian government or@@ dered all c@@ ine@@ mas to be closed , for up to a month .\n","one pur@@ po@@ se and gu@@ i@@ ding pr@@ inci@@ ple of the pre@@ ce@@ de@@ –@@ proce@@ ed mo@@ del is to direc@@ t initi@@ al atten@@ tion to out@@ comes , ra@@ ther than in@@ pu@@ ts .\n","since 199@@ 6 , the net worth of people under 35 has dro@@ pped by more than 34 percent .\n","the best example of e@@ cho@@ loc@@ ation is found in bat@@ s .\n","\"@@ in a statement to the j@@ our@@ nal na@@ ture bio@@ technology in february 2020 , us national institu@@ tes of health vir@@ al e@@ co@@ logy un@@ it chief v@@ inc@@ ent m@@ un@@ ster said , \" \"the general gen@@ o@@ mic la@@ y@@ out and the general re@@ p@@ lic@@ ation k@@ ine@@ tics and the bio@@ logy of the mers , sars and [@@ sars -cov -2 ] vi@@ rus@@ es are very similar , so testing dru@@ gs which tar@@ get relati@@ vely gener@@ ic parts of these corona@@ vi@@ rus@@ es is a logical ste@@ p \" \" . \"\n","vi@@ p korean dr@@ ama star@@ ring j@@ ang na -@@ ra and le@@ e sang -@@ yo@@ on\n","instead , they allowed the establi@@ sh@@ ment of a jo@@ int -@@ stock company in 201@@ 0 to car@@ ry out the project following requ@@ ests from the ministry of industry and trade , un@@ its be@@ long@@ ing to who had been le@@ asing the land .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g3uCvTzuyEqo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003717530,"user_tz":-420,"elapsed":402,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"903e9835-0eca-4786-8581-462cd748e789"},"source":["! head data/en-vi/dev.4000.bpe.vi"],"execution_count":18,"outputs":[{"output_type":"stream","text":["13 trường hợp bao gồm một cá nhân mỗi người từ thái lan , trung quốc , mor@@ oc@@ co và ấn độ , hai cá nhân từ ả r@@ ập sau@@ di , e@@ thi@@ op@@ ia , iran và ba cá nhân từ u@@ a@@ e đã được phát hiện thông qua báo cáo sớm của các cá nhân .\n","do đó , người hàn quốc đã mu@@ ối ra@@ u trái làm thức ăn dự tr@@ ữ cho mùa đông .\n","\"@@ mu@@ ̀@@ a trước , một cổ động viên gi@@ ận dữ gọi điện đến cảnh sát no@@ t@@ ti@@ ng@@ ha@@ m@@ shi@@ re thông báo rằng chiếc th@@ ẻ đỏ của n@@ an@@ i trong trận thuộc giải ch@@ amp@@ i@@ ons lea@@ gue với real mad@@ ri@@ d là một \" \"@@ tội ác \" \" . \"\n","sau đó , do hậu quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý đã ra lệnh đóng cửa tất cả các r@@ ạp chiếu phim , trong vòng một tháng .\n","một mục đích và nguyên t@@ ắc chỉ đạo của mô hình pre@@ ce@@ de - proce@@ ed là để hướng sự chú ý ban đầu đến kết quả , ch@@ ứ không phải là yếu tố đầu vào .\n","kể từ năm 199@@ 6 , khối tài sản r@@ òng của những người dưới 35 tuổi đã giảm xuống hơn 34 % .\n","ví dụ tốt nhất của định vị bằng tiếng v@@ ang là ở loài d@@ ơi .\n","\"@@ trong một tuyên bố với tạp chí công nghệ sinh học tự nhiên vào tháng 2 năm 2020 , giám đốc bộ phận sinh thái virus của viện sức khỏe quốc gia hoa kỳ v@@ inc@@ ent m@@ un@@ ster nói : \" \"@@ bố cục gen@@ e nói chung và động lực nhân rộng chung và sinh học của mers , sars và [@@ sars -cov -2 ] virus rất giống nhau , vì vậy các loại thuốc thử nghiệm nh@@ ắm vào các phần tương đối chung của các coronavirus này là một bước đi hợp lý \" \" . \"\n","bộ phim truyền hình vi@@ p hàn quốc với sự tham gia của j@@ ang na -@@ ra và le@@ e sang -@@ yo@@ on\n","thay vào đó , họ cho phép thành lập một công ty cổ phần vào năm 201@@ 0 để thực hiện dự án theo yêu cầu của bộ công thương , các đơn vị đã thuê đất .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7jS8RHLZyKKf"},"source":["## Prepare the vocabulary"]},{"cell_type":"markdown","metadata":{"id":"ndkGOp7F3LWY"},"source":["From the pre-processed training data, we extract the final vocabulary for the translation model. It should contain all subwords needed for representing the source and target training data."]},{"cell_type":"code","metadata":{"id":"AGq8KMqjySXm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627003723595,"user_tz":-420,"elapsed":377,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"d5c46747-bbaf-470a-c2ce-f9a48b60ca93"},"source":["! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py"],"execution_count":19,"outputs":[{"output_type":"stream","text":["--2021-07-23 01:30:04--  https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2034 (2.0K) [text/plain]\n","Saving to: ‘build_vocab.py’\n","\n","\rbuild_vocab.py        0%[                    ]       0  --.-KB/s               \rbuild_vocab.py      100%[===================>]   1.99K  --.-KB/s    in 0s      \n","\n","2021-07-23 01:30:05 (34.8 MB/s) - ‘build_vocab.py’ saved [2034/2034]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k1_iQEQEyJyS","executionInfo":{"status":"ok","timestamp":1627003728314,"user_tz":-420,"elapsed":938,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["vocab_src_file = src_bpe_files['train']\n","vocab_trg_file = trg_bpe_files['train']\n","bpe_vocab_file = os.path.join(datadir, f'joint.{bpe_size}bpe.vocab')\n","\n","! python build_vocab.py  \\\n","  $vocab_src_file $vocab_trg_file \\\n","  --output_path $bpe_vocab_file"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XSuYid3JdECc"},"source":["# Model configuration"]},{"cell_type":"markdown","metadata":{"id":"45uGP83v3Y24"},"source":["Joey NMT reads model and training hyperparameters from a configuration file. We're generating this now to configure paths in the appropriate places. \n","\n","The configuration below builds a small Transformer model with shared embeddings between source and target language on the base of the subword vocabularies created above."]},{"cell_type":"code","metadata":{"id":"EirEmJmkc7sx","executionInfo":{"status":"ok","timestamp":1627003746649,"user_tz":-420,"elapsed":375,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}}},"source":["# Create the config\n","config = \"\"\"\n","name: \"{name}_transformer\"\n","\n","data:\n","    src: \"{source_language}\"\n","    trg: \"{target_language}\"\n","    train: \"{datadir}/train.{bpe_size}.bpe\"\n","    dev:   \"{datadir}/dev.{bpe_size}.bpe\"\n","    test:  \"{datadir}/test.{bpe_size}.bpe\"\n","    level: \"bpe\"\n","    lowercase: False                \n","    max_sent_length: 300             # Extend to longer sentences.\n","    src_vocab: \"{vocab_src_file}\"\n","    trg_vocab: \"{vocab_trg_file}\"\n","\n","testing:\n","    beam_size: 5\n","    alpha: 1.0\n","    sacrebleu:                      # sacrebleu options\n","        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n","        tokenize: \"intl\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n","\n","training:\n","    #load_model: \"models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n","    random_seed: 42\n","    optimizer: \"adam\"\n","    normalization: \"tokens\"\n","    adam_betas: [0.9, 0.999] \n","    scheduling: \"plateau\"           # Alternative: try switching from plateau to Noam scheduling\n","    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n","    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n","    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n","    decrease_factor: 0.7\n","    loss: \"crossentropy\"\n","    learning_rate: 0.0003\n","    learning_rate_min: 0.00000001\n","    weight_decay: 0.0\n","    label_smoothing: 0.1\n","    batch_size: 4096\n","    batch_type: \"token\"\n","    eval_batch_size: 3600\n","    eval_batch_type: \"token\"\n","    batch_multiplier: 1\n","    early_stopping_metric: \"ppl\"\n","    epochs: 32                     # Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n","    validation_freq: 2000          # Set to at least once per epoch.\n","    logging_freq: 200\n","    eval_metric: \"bleu\"\n","    model_dir: \"models/{name}_transformer\"\n","    overwrite: False               # Set to True if you want to overwrite possibly existing models. \n","    shuffle: True\n","    use_cuda: True\n","    max_output_length: 300\n","    print_valid_sents: [0, 1, 2, 3]\n","    keep_last_ckpts: 3\n","\n","model:\n","    initializer: \"xavier\"\n","    bias_initializer: \"zeros\"\n","    init_gain: 1.0\n","    embed_initializer: \"xavier\"\n","    embed_init_gain: 1.0\n","    tied_embeddings: True       # Requires joint vocabulary.\n","    tied_softmax: True\n","    encoder:\n","        type: \"transformer\"\n","        num_layers: 2\n","        num_heads: 2            # Increase to 8 for larger data.\n","        embeddings:\n","            embedding_dim: 512   # Increase to 512 for larger data.\n","            scale: True\n","            dropout: 0.2\n","        # typically ff_size = 4 x hidden_size\n","        hidden_size: 512         # Increase to 512 for larger data.\n","        ff_size: 1024            # Increase to 2048 for larger data.\n","        dropout: 0.3\n","    decoder:\n","        type: \"transformer\"\n","        num_layers: 2\n","        num_heads: 2              # Increase to 8 for larger data.\n","        embeddings:\n","            embedding_dim: 512    # Increase to 512 for larger data.\n","            scale: True\n","            dropout: 0.2\n","        # typically ff_size = 4 x hidden_size\n","        hidden_size: 512         # TODO: Increase to 512 for larger data.\n","        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n","        dropout: 0.3\n","\"\"\".format(name=name, source_language=src_lang, target_language=trg_lang,\n","           datadir=datadir, vocab_src_file=bpe_vocab_file, \n","           vocab_trg_file=bpe_vocab_file, bpe_size=bpe_size)\n","with open(\"transformer_{name}.yaml\".format(name=name),'w') as f:\n","    f.write(config)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIOosBx1fDIQ"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"D20-6ecg4PvC"},"source":["This will take a while. The log reports the training process, look out for the prints of example translations and the BLEU evaluation scores to get an impression of the current quality. \n","\n","The log is also stored in the model directory within this runtime (inspect files in the menu on the left). There you can also find a summary report of all validations. We'll also use TensorBoard to visualize the training progress on the go. This requires enabling Cookies in the browser.\n","\n","After 12h at the latest, Colab will disconnect, so to make sure you're progress is not lost, download the checkpoints from the model directory from time to time. You'll later be able to reload them if model hyperparameters match."]},{"cell_type":"code","metadata":{"id":"FF9do6ohedY6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627005503523,"user_tz":-420,"elapsed":1747546,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"bc5ea067-e48f-4087-acc0-db58bebe125b"},"source":["!python -m joeynmt train transformer_en_vi_bpe4000.yaml"],"execution_count":22,"outputs":[{"output_type":"stream","text":["2021-07-23 01:30:39,281 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n","2021-07-23 01:30:39,343 - INFO - joeynmt.data - Loading training data...\n","2021-07-23 01:30:39,773 - INFO - joeynmt.data - Building vocabulary...\n","2021-07-23 01:30:40,085 - INFO - joeynmt.data - Loading dev data...\n","2021-07-23 01:30:40,100 - INFO - joeynmt.data - Loading test data...\n","2021-07-23 01:30:40,123 - INFO - joeynmt.data - Data loaded.\n","2021-07-23 01:30:40,124 - INFO - joeynmt.model - Building an encoder-decoder model...\n","2021-07-23 01:30:40,367 - INFO - joeynmt.model - Enc-dec model built.\n","2021-07-23 01:30:40.607120: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","2021-07-23 01:30:42,410 - INFO - joeynmt.training - Total params: 12763648\n","2021-07-23 01:30:46,024 - INFO - joeynmt.helpers - cfg.name                           : en_vi_bpe4000_transformer\n","2021-07-23 01:30:46,024 - INFO - joeynmt.helpers - cfg.data.src                       : en\n","2021-07-23 01:30:46,024 - INFO - joeynmt.helpers - cfg.data.trg                       : vi\n","2021-07-23 01:30:46,024 - INFO - joeynmt.helpers - cfg.data.train                     : /content/data/en-vi//train.4000.bpe\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/data/en-vi//dev.4000.bpe\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.data.test                      : /content/data/en-vi//test.4000.bpe\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 300\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/data/en-vi/joint.4000bpe.vocab\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/data/en-vi/joint.4000bpe.vocab\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.tokenize     : intl\n","2021-07-23 01:30:46,025 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n","2021-07-23 01:30:46,026 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.epochs                : 32\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/en_vi_bpe4000_transformer\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 300\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n","2021-07-23 01:30:46,027 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 2\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 2\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n","2021-07-23 01:30:46,028 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 2\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 2\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - Data set sizes: \n","\ttrain 19998,\n","\tvalid 1007,\n","\ttest 1220\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - First training example:\n","\t[SRC] hu@@ ng@@ er is a possible reas@@ on that you@@ ’@@ re in a ba@@ d mo@@ o@@ d , especially if you@@ ’@@ re str@@ es@@ sed out and too bus@@ y to find time to e@@ at .\n","\t[TRG] đ@@ ói cũng là lý do làm bạn b@@ ực b@@ ội khó chịu , nhất là nếu bạn c@@ ăng thẳng và b@@ ận r@@ ộn đến n@@ ỗi không có thời gian để ăn .\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) and\n","2021-07-23 01:30:46,029 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) and\n","2021-07-23 01:30:46,030 - INFO - joeynmt.helpers - Number of Src words (types): 4389\n","2021-07-23 01:30:46,030 - INFO - joeynmt.helpers - Number of Trg words (types): 4389\n","2021-07-23 01:30:46,030 - INFO - joeynmt.training - Model(\n","\tencoder=TransformerEncoder(num_layers=2, num_heads=2),\n","\tdecoder=TransformerDecoder(num_layers=2, num_heads=2),\n","\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=4389),\n","\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=4389))\n","2021-07-23 01:30:46,034 - INFO - joeynmt.training - Train stats:\n","\tdevice: cuda\n","\tn_gpu: 1\n","\t16-bits training: False\n","\tgradient accumulation: 1\n","\tbatch size per device: 4096\n","\ttotal batch size (w. parallel & accumulation): 4096\n","2021-07-23 01:30:46,034 - INFO - joeynmt.training - EPOCH 1\n","2021-07-23 01:31:05,389 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.452498, Tokens per Sec:    25096, Lr: 0.000300\n","2021-07-23 01:31:16,937 - INFO - joeynmt.training - Epoch   1: total training loss 1732.74\n","2021-07-23 01:31:16,938 - INFO - joeynmt.training - EPOCH 2\n","2021-07-23 01:31:24,972 - INFO - joeynmt.training - Epoch   2, Step:      400, Batch Loss:     4.422051, Tokens per Sec:    24314, Lr: 0.000300\n","2021-07-23 01:31:45,001 - INFO - joeynmt.training - Epoch   2, Step:      600, Batch Loss:     3.927940, Tokens per Sec:    24282, Lr: 0.000300\n","2021-07-23 01:31:48,803 - INFO - joeynmt.training - Epoch   2: total training loss 1373.00\n","2021-07-23 01:31:48,803 - INFO - joeynmt.training - EPOCH 3\n","2021-07-23 01:32:05,454 - INFO - joeynmt.training - Epoch   3, Step:      800, Batch Loss:     4.002846, Tokens per Sec:    23232, Lr: 0.000300\n","2021-07-23 01:32:22,266 - INFO - joeynmt.training - Epoch   3: total training loss 1255.10\n","2021-07-23 01:32:22,267 - INFO - joeynmt.training - EPOCH 4\n","2021-07-23 01:32:26,519 - INFO - joeynmt.training - Epoch   4, Step:     1000, Batch Loss:     3.950848, Tokens per Sec:    22460, Lr: 0.000300\n","2021-07-23 01:32:48,570 - INFO - joeynmt.training - Epoch   4, Step:     1200, Batch Loss:     3.592678, Tokens per Sec:    22006, Lr: 0.000300\n","2021-07-23 01:32:57,344 - INFO - joeynmt.training - Epoch   4: total training loss 1144.51\n","2021-07-23 01:32:57,345 - INFO - joeynmt.training - EPOCH 5\n","2021-07-23 01:33:10,348 - INFO - joeynmt.training - Epoch   5, Step:     1400, Batch Loss:     3.458529, Tokens per Sec:    22834, Lr: 0.000300\n","2021-07-23 01:33:31,339 - INFO - joeynmt.training - Epoch   5, Step:     1600, Batch Loss:     3.000442, Tokens per Sec:    22575, Lr: 0.000300\n","2021-07-23 01:33:31,446 - INFO - joeynmt.training - Epoch   5: total training loss 1074.18\n","2021-07-23 01:33:31,446 - INFO - joeynmt.training - EPOCH 6\n","2021-07-23 01:33:52,438 - INFO - joeynmt.training - Epoch   6, Step:     1800, Batch Loss:     2.969010, Tokens per Sec:    22643, Lr: 0.000300\n","2021-07-23 01:34:05,715 - INFO - joeynmt.training - Epoch   6: total training loss 1023.85\n","2021-07-23 01:34:05,715 - INFO - joeynmt.training - EPOCH 7\n","2021-07-23 01:34:14,044 - INFO - joeynmt.training - Epoch   7, Step:     2000, Batch Loss:     3.013360, Tokens per Sec:    21835, Lr: 0.000300\n","2021-07-23 01:37:51,782 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 01:37:51,783 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 01:37:51,783 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 01:37:53,459 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n","2021-07-23 01:37:53,460 - INFO - joeynmt.training - Saving new checkpoint.\n","2021-07-23 01:37:53,873 - INFO - joeynmt.training - Example #0\n","2021-07-23 01:37:53,873 - INFO - joeynmt.training - \tSource:     13 cases consisted of one individual each from thailand , china , morocco and india , two individuals each from saudi arabia , ethiopia , iran and three individuals from the uae were detected through early reporting by individuals .\n","2021-07-23 01:37:53,873 - INFO - joeynmt.training - \tReference:  13 trường hợp bao gồm một cá nhân mỗi người từ thái lan , trung quốc , morocco và ấn độ , hai cá nhân từ ả rập saudi , ethiopia , iran và ba cá nhân từ uae đã được phát hiện thông qua báo cáo sớm của các cá nhân .\n","2021-07-23 01:37:53,873 - INFO - joeynmt.training - \tHypothesis: 13 trường hợp được phát hiện là một trong một người dân từ thái lan , trung quốc và trung quốc , hai cá nhân , và hai cá nhân , bao gồm cả iran , iran , iran và iran đã được phát hiện qua các cá nhân đã được phát hiện qua qua các cá nhân .\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - Example #1\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - \tSource:     therefore , koreans have fermented vegetables as food reserves for the winter .\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - \tReference:  do đó , người hàn quốc đã muối rau trái làm thức ăn dự trữ cho mùa đông .\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - \tHypothesis: do đó , người dùng đã thu hút hàng trăm người dùng dùng để mua săn ăn ăn cho mùa đông .\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - Example #2\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - \tSource:     last season , an incensed fan phoned nottinghamshire police reporting nani 's red card in the champions league match against real madrid as a 'crime ' .\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - \tReference:  \"mùa trước , một cổ động viên giận dữ gọi điện đến cảnh sát nottinghamshire thông báo rằng chiếc thẻ đỏ của nani trong trận thuộc giải champions league với real madrid là một \" \"tội ác \" \" . \"\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - \tHypothesis: mùa đông , một cuộc họp báo đã được đưa ra bởi cảnh sát sát sát sát sát sát sát sát sát sát sát sát sát cho biết vào cuộc họp của cuộc họp báo về việc giải quyết vấn đề về các nhà thách thức như một cá nhân .\n","2021-07-23 01:37:53,874 - INFO - joeynmt.training - Example #3\n","2021-07-23 01:37:53,875 - INFO - joeynmt.training - \tSource:     later , as a result of the pandemic in italy , on march 8 , 2020 the italian government ordered all cinemas to be closed , for up to a month .\n","2021-07-23 01:37:53,875 - INFO - joeynmt.training - \tReference:  sau đó , do hậu quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý đã ra lệnh đóng cửa tất cả các rạp chiếu phim , trong vòng một tháng .\n","2021-07-23 01:37:53,875 - INFO - joeynmt.training - \tHypothesis: sau đó , kết quả của đại dịch covid -19 , ý vào ngày 8 tháng 3 năm 2020 , chính phủ ý đã đóng cửa tất cả các cửa biên giới , đã đóng cửa vào tháng trước .\n","2021-07-23 01:37:53,875 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     2000: bleu:  12.23, loss: 116175.7266, ppl:  19.6310, duration: 219.8302s\n","2021-07-23 01:38:15,404 - INFO - joeynmt.training - Epoch   7, Step:     2200, Batch Loss:     3.050779, Tokens per Sec:    22664, Lr: 0.000300\n","2021-07-23 01:38:19,975 - INFO - joeynmt.training - Epoch   7: total training loss 968.60\n","2021-07-23 01:38:19,975 - INFO - joeynmt.training - EPOCH 8\n","2021-07-23 01:38:37,133 - INFO - joeynmt.training - Epoch   8, Step:     2400, Batch Loss:     2.973241, Tokens per Sec:    22415, Lr: 0.000300\n","2021-07-23 01:38:54,406 - INFO - joeynmt.training - Epoch   8: total training loss 932.79\n","2021-07-23 01:38:54,406 - INFO - joeynmt.training - EPOCH 9\n","2021-07-23 01:38:58,468 - INFO - joeynmt.training - Epoch   9, Step:     2600, Batch Loss:     2.858478, Tokens per Sec:    22559, Lr: 0.000300\n","2021-07-23 01:39:19,854 - INFO - joeynmt.training - Epoch   9, Step:     2800, Batch Loss:     2.579842, Tokens per Sec:    22323, Lr: 0.000300\n","2021-07-23 01:39:28,887 - INFO - joeynmt.training - Epoch   9: total training loss 905.46\n","2021-07-23 01:39:28,887 - INFO - joeynmt.training - EPOCH 10\n","2021-07-23 01:39:41,135 - INFO - joeynmt.training - Epoch  10, Step:     3000, Batch Loss:     2.924365, Tokens per Sec:    22063, Lr: 0.000300\n","2021-07-23 01:40:02,890 - INFO - joeynmt.training - Epoch  10, Step:     3200, Batch Loss:     2.917153, Tokens per Sec:    22795, Lr: 0.000300\n","2021-07-23 01:40:03,216 - INFO - joeynmt.training - Epoch  10: total training loss 863.60\n","2021-07-23 01:40:03,216 - INFO - joeynmt.training - EPOCH 11\n","2021-07-23 01:40:24,468 - INFO - joeynmt.training - Epoch  11, Step:     3400, Batch Loss:     2.043476, Tokens per Sec:    22383, Lr: 0.000300\n","2021-07-23 01:40:37,763 - INFO - joeynmt.training - Epoch  11: total training loss 840.48\n","2021-07-23 01:40:37,764 - INFO - joeynmt.training - EPOCH 12\n","2021-07-23 01:40:46,108 - INFO - joeynmt.training - Epoch  12, Step:     3600, Batch Loss:     3.130423, Tokens per Sec:    22690, Lr: 0.000300\n","2021-07-23 01:41:07,737 - INFO - joeynmt.training - Epoch  12, Step:     3800, Batch Loss:     2.453705, Tokens per Sec:    22398, Lr: 0.000300\n","2021-07-23 01:41:12,165 - INFO - joeynmt.training - Epoch  12: total training loss 816.24\n","2021-07-23 01:41:12,166 - INFO - joeynmt.training - EPOCH 13\n","2021-07-23 01:41:29,290 - INFO - joeynmt.training - Epoch  13, Step:     4000, Batch Loss:     2.517236, Tokens per Sec:    22463, Lr: 0.000300\n","2021-07-23 01:43:05,817 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 01:43:05,818 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 01:43:05,818 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 01:43:07,306 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n","2021-07-23 01:43:07,306 - INFO - joeynmt.training - Saving new checkpoint.\n","2021-07-23 01:43:07,720 - INFO - joeynmt.training - Example #0\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - \tSource:     13 cases consisted of one individual each from thailand , china , morocco and india , two individuals each from saudi arabia , ethiopia , iran and three individuals from the uae were detected through early reporting by individuals .\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - \tReference:  13 trường hợp bao gồm một cá nhân mỗi người từ thái lan , trung quốc , morocco và ấn độ , hai cá nhân từ ả rập saudi , ethiopia , iran và ba cá nhân từ uae đã được phát hiện thông qua báo cáo sớm của các cá nhân .\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - \tHypothesis: 13 trường hợp được ghi nhận trong một cá nhân từ thái lan , trung quốc , trung quốc và hai cá nhân từ ả rập saudi , ethiopia , iran và ba cá nhân từ ue , iran và ba cá nhân được phát hiện bởi các cá nhân được báo cáo .\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - Example #1\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - \tSource:     therefore , koreans have fermented vegetables as food reserves for the winter .\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - \tReference:  do đó , người hàn quốc đã muối rau trái làm thức ăn dự trữ cho mùa đông .\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - \tHypothesis: do đó , hàn quốc đã báo cáo rau u u u củ lạnh như thực phẩm thực phẩm cho mùa đông .\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - Example #2\n","2021-07-23 01:43:07,721 - INFO - joeynmt.training - \tSource:     last season , an incensed fan phoned nottinghamshire police reporting nani 's red card in the champions league match against real madrid as a 'crime ' .\n","2021-07-23 01:43:07,722 - INFO - joeynmt.training - \tReference:  \"mùa trước , một cổ động viên giận dữ gọi điện đến cảnh sát nottinghamshire thông báo rằng chiếc thẻ đỏ của nani trong trận thuộc giải champions league với real madrid là một \" \"tội ác \" \" . \"\n","2021-07-23 01:43:07,722 - INFO - joeynmt.training - \tHypothesis: mùa ngoái , một người hâm mộ đã đề cập đến cảnh sát cảnh sát đã cảnh sát cảnh sát cho biết rằng chiếc xe địch ịch ịch ịch ịch ịch ịch ịch ịch ịch bất bất cứ ai ai nào nào đó .\n","2021-07-23 01:43:07,722 - INFO - joeynmt.training - Example #3\n","2021-07-23 01:43:07,722 - INFO - joeynmt.training - \tSource:     later , as a result of the pandemic in italy , on march 8 , 2020 the italian government ordered all cinemas to be closed , for up to a month .\n","2021-07-23 01:43:07,722 - INFO - joeynmt.training - \tReference:  sau đó , do hậu quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý đã ra lệnh đóng cửa tất cả các rạp chiếu phim , trong vòng một tháng .\n","2021-07-23 01:43:07,722 - INFO - joeynmt.training - \tHypothesis: sau đó , kết quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý lệnh tất cả các kênh đóng cửa , đã đóng cửa , để tăng cường một tháng .\n","2021-07-23 01:43:07,722 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step     4000: bleu:  23.73, loss: 96497.7188, ppl:  11.8561, duration: 98.4313s\n","2021-07-23 01:43:25,024 - INFO - joeynmt.training - Epoch  13: total training loss 794.56\n","2021-07-23 01:43:25,024 - INFO - joeynmt.training - EPOCH 14\n","2021-07-23 01:43:29,232 - INFO - joeynmt.training - Epoch  14, Step:     4200, Batch Loss:     2.437185, Tokens per Sec:    22780, Lr: 0.000300\n","2021-07-23 01:43:50,754 - INFO - joeynmt.training - Epoch  14, Step:     4400, Batch Loss:     2.251608, Tokens per Sec:    22106, Lr: 0.000300\n","2021-07-23 01:43:59,724 - INFO - joeynmt.training - Epoch  14: total training loss 785.02\n","2021-07-23 01:43:59,724 - INFO - joeynmt.training - EPOCH 15\n","2021-07-23 01:44:12,522 - INFO - joeynmt.training - Epoch  15, Step:     4600, Batch Loss:     2.046962, Tokens per Sec:    22497, Lr: 0.000300\n","2021-07-23 01:44:34,075 - INFO - joeynmt.training - Epoch  15: total training loss 753.16\n","2021-07-23 01:44:34,075 - INFO - joeynmt.training - EPOCH 16\n","2021-07-23 01:44:34,217 - INFO - joeynmt.training - Epoch  16, Step:     4800, Batch Loss:     2.013197, Tokens per Sec:    19170, Lr: 0.000300\n","2021-07-23 01:44:55,946 - INFO - joeynmt.training - Epoch  16, Step:     5000, Batch Loss:     2.229571, Tokens per Sec:    22587, Lr: 0.000300\n","2021-07-23 01:45:08,497 - INFO - joeynmt.training - Epoch  16: total training loss 740.53\n","2021-07-23 01:45:08,498 - INFO - joeynmt.training - EPOCH 17\n","2021-07-23 01:45:17,630 - INFO - joeynmt.training - Epoch  17, Step:     5200, Batch Loss:     2.422415, Tokens per Sec:    22658, Lr: 0.000300\n","2021-07-23 01:45:39,257 - INFO - joeynmt.training - Epoch  17, Step:     5400, Batch Loss:     2.310730, Tokens per Sec:    22383, Lr: 0.000300\n","2021-07-23 01:45:42,906 - INFO - joeynmt.training - Epoch  17: total training loss 724.73\n","2021-07-23 01:45:42,906 - INFO - joeynmt.training - EPOCH 18\n","2021-07-23 01:46:00,613 - INFO - joeynmt.training - Epoch  18, Step:     5600, Batch Loss:     2.158442, Tokens per Sec:    22436, Lr: 0.000300\n","2021-07-23 01:46:17,400 - INFO - joeynmt.training - Epoch  18: total training loss 717.37\n","2021-07-23 01:46:17,400 - INFO - joeynmt.training - EPOCH 19\n","2021-07-23 01:46:22,371 - INFO - joeynmt.training - Epoch  19, Step:     5800, Batch Loss:     1.975981, Tokens per Sec:    22469, Lr: 0.000300\n","2021-07-23 01:46:43,945 - INFO - joeynmt.training - Epoch  19, Step:     6000, Batch Loss:     2.724646, Tokens per Sec:    22736, Lr: 0.000300\n","2021-07-23 01:48:07,833 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 01:48:07,833 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 01:48:07,833 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 01:48:09,312 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n","2021-07-23 01:48:09,312 - INFO - joeynmt.training - Saving new checkpoint.\n","2021-07-23 01:48:09,683 - INFO - joeynmt.training - Example #0\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tSource:     13 cases consisted of one individual each from thailand , china , morocco and india , two individuals each from saudi arabia , ethiopia , iran and three individuals from the uae were detected through early reporting by individuals .\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tReference:  13 trường hợp bao gồm một cá nhân mỗi người từ thái lan , trung quốc , morocco và ấn độ , hai cá nhân từ ả rập saudi , ethiopia , iran và ba cá nhân từ uae đã được phát hiện thông qua báo cáo sớm của các cá nhân .\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tHypothesis: 13 trường hợp liên quan đến một trong số một cá nhân từ thái lan , trung quốc , trung quốc và ấn độ , hai cá nhân từ ả rập saudi arabia , opia , iran và ba cá nhân từ uae được phát hiện qua các cá nhân .\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - Example #1\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tSource:     therefore , koreans have fermented vegetables as food reserves for the winter .\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tReference:  do đó , người hàn quốc đã muối rau trái làm thức ăn dự trữ cho mùa đông .\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tHypothesis: do đó , người hàn quốc đã báo cáo rau rau được món ăn ăn để phục vụ mùa đông .\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - Example #2\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tSource:     last season , an incensed fan phoned nottinghamshire police reporting nani 's red card in the champions league match against real madrid as a 'crime ' .\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tReference:  \"mùa trước , một cổ động viên giận dữ gọi điện đến cảnh sát nottinghamshire thông báo rằng chiếc thẻ đỏ của nani trong trận thuộc giải champions league với real madrid là một \" \"tội ác \" \" . \"\n","2021-07-23 01:48:09,684 - INFO - joeynmt.training - \tHypothesis: mùa trước , một incenan đã hoãn lại một thông báo đã cảnh sát cảnh sát cảnh sát đã báo báo cáo rằng nani đã địch ịch aff cup đối với madrid là một nhà lãnh đạo .\n","2021-07-23 01:48:09,685 - INFO - joeynmt.training - Example #3\n","2021-07-23 01:48:09,685 - INFO - joeynmt.training - \tSource:     later , as a result of the pandemic in italy , on march 8 , 2020 the italian government ordered all cinemas to be closed , for up to a month .\n","2021-07-23 01:48:09,685 - INFO - joeynmt.training - \tReference:  sau đó , do hậu quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý đã ra lệnh đóng cửa tất cả các rạp chiếu phim , trong vòng một tháng .\n","2021-07-23 01:48:09,685 - INFO - joeynmt.training - \tHypothesis: sau đó , kết quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý lệnh tất cả các rạp chiếu phim sẽ bị đóng cửa , để tăng lên một tháng .\n","2021-07-23 01:48:09,685 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     6000: bleu:  28.62, loss: 87491.7500, ppl:   9.4127, duration: 85.7393s\n","2021-07-23 01:48:17,475 - INFO - joeynmt.training - Epoch  19: total training loss 702.31\n","2021-07-23 01:48:17,475 - INFO - joeynmt.training - EPOCH 20\n","2021-07-23 01:48:31,384 - INFO - joeynmt.training - Epoch  20, Step:     6200, Batch Loss:     2.336682, Tokens per Sec:    22262, Lr: 0.000300\n","2021-07-23 01:48:51,988 - INFO - joeynmt.training - Epoch  20: total training loss 691.89\n","2021-07-23 01:48:51,988 - INFO - joeynmt.training - EPOCH 21\n","2021-07-23 01:48:53,097 - INFO - joeynmt.training - Epoch  21, Step:     6400, Batch Loss:     2.437260, Tokens per Sec:    22589, Lr: 0.000300\n","2021-07-23 01:49:14,895 - INFO - joeynmt.training - Epoch  21, Step:     6600, Batch Loss:     1.901477, Tokens per Sec:    22721, Lr: 0.000300\n","2021-07-23 01:49:26,196 - INFO - joeynmt.training - Epoch  21: total training loss 671.43\n","2021-07-23 01:49:26,196 - INFO - joeynmt.training - EPOCH 22\n","2021-07-23 01:49:36,624 - INFO - joeynmt.training - Epoch  22, Step:     6800, Batch Loss:     2.474009, Tokens per Sec:    22661, Lr: 0.000300\n","2021-07-23 01:49:58,084 - INFO - joeynmt.training - Epoch  22, Step:     7000, Batch Loss:     1.996601, Tokens per Sec:    22588, Lr: 0.000300\n","2021-07-23 01:50:00,495 - INFO - joeynmt.training - Epoch  22: total training loss 673.19\n","2021-07-23 01:50:00,495 - INFO - joeynmt.training - EPOCH 23\n","2021-07-23 01:50:19,462 - INFO - joeynmt.training - Epoch  23, Step:     7200, Batch Loss:     2.130306, Tokens per Sec:    22432, Lr: 0.000300\n","2021-07-23 01:50:34,989 - INFO - joeynmt.training - Epoch  23: total training loss 670.34\n","2021-07-23 01:50:34,989 - INFO - joeynmt.training - EPOCH 24\n","2021-07-23 01:50:40,932 - INFO - joeynmt.training - Epoch  24, Step:     7400, Batch Loss:     1.463538, Tokens per Sec:    22450, Lr: 0.000300\n","2021-07-23 01:51:02,529 - INFO - joeynmt.training - Epoch  24, Step:     7600, Batch Loss:     2.092011, Tokens per Sec:    22474, Lr: 0.000300\n","2021-07-23 01:51:09,397 - INFO - joeynmt.training - Epoch  24: total training loss 657.42\n","2021-07-23 01:51:09,397 - INFO - joeynmt.training - EPOCH 25\n","2021-07-23 01:51:24,105 - INFO - joeynmt.training - Epoch  25, Step:     7800, Batch Loss:     1.895654, Tokens per Sec:    22504, Lr: 0.000300\n","2021-07-23 01:51:43,729 - INFO - joeynmt.training - Epoch  25: total training loss 646.59\n","2021-07-23 01:51:43,729 - INFO - joeynmt.training - EPOCH 26\n","2021-07-23 01:51:45,652 - INFO - joeynmt.training - Epoch  26, Step:     8000, Batch Loss:     1.915653, Tokens per Sec:    22465, Lr: 0.000300\n","2021-07-23 01:53:28,035 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 01:53:28,035 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 01:53:28,035 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 01:53:29,655 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n","2021-07-23 01:53:29,655 - INFO - joeynmt.training - Saving new checkpoint.\n","2021-07-23 01:53:30,089 - INFO - joeynmt.training - Example #0\n","2021-07-23 01:53:30,090 - INFO - joeynmt.training - \tSource:     13 cases consisted of one individual each from thailand , china , morocco and india , two individuals each from saudi arabia , ethiopia , iran and three individuals from the uae were detected through early reporting by individuals .\n","2021-07-23 01:53:30,090 - INFO - joeynmt.training - \tReference:  13 trường hợp bao gồm một cá nhân mỗi người từ thái lan , trung quốc , morocco và ấn độ , hai cá nhân từ ả rập saudi , ethiopia , iran và ba cá nhân từ uae đã được phát hiện thông qua báo cáo sớm của các cá nhân .\n","2021-07-23 01:53:30,090 - INFO - joeynmt.training - \tHypothesis: 13 trường hợp được phát hiện vào ngày 13 tháng 3 , trung quốc , nhật bản và ấn độ , hai cá nhân từ ả rập saudi , ethiopia , iran và ba cá nhân từ uae được phát hiện đầu bởi các cá nhân .\n","2021-07-23 01:53:30,090 - INFO - joeynmt.training - Example #1\n","2021-07-23 01:53:30,090 - INFO - joeynmt.training - \tSource:     therefore , koreans have fermented vegetables as food reserves for the winter .\n","2021-07-23 01:53:30,090 - INFO - joeynmt.training - \tReference:  do đó , người hàn quốc đã muối rau trái làm thức ăn dự trữ cho mùa đông .\n","2021-07-23 01:53:30,090 - INFO - joeynmt.training - \tHypothesis: do đó , hàn quốc đã phân loại rau quả như thực phẩm ăn cho mùa đông .\n","2021-07-23 01:53:30,090 - INFO - joeynmt.training - Example #2\n","2021-07-23 01:53:30,091 - INFO - joeynmt.training - \tSource:     last season , an incensed fan phoned nottinghamshire police reporting nani 's red card in the champions league match against real madrid as a 'crime ' .\n","2021-07-23 01:53:30,091 - INFO - joeynmt.training - \tReference:  \"mùa trước , một cổ động viên giận dữ gọi điện đến cảnh sát nottinghamshire thông báo rằng chiếc thẻ đỏ của nani trong trận thuộc giải champions league với real madrid là một \" \"tội ác \" \" . \"\n","2021-07-23 01:53:30,091 - INFO - joeynmt.training - \tHypothesis: mùa cuối tuần trước , một incenfan đã đề cập đến cuộc đàm phán quyết của cảnh sát đã đăng tải tải tại giải vô địch ịch ịch trận đấu với thực sự đối với madrid là một 'tội phạm . ' , 'tội phạm . ' , 'tội phạm .\n","2021-07-23 01:53:30,091 - INFO - joeynmt.training - Example #3\n","2021-07-23 01:53:30,091 - INFO - joeynmt.training - \tSource:     later , as a result of the pandemic in italy , on march 8 , 2020 the italian government ordered all cinemas to be closed , for up to a month .\n","2021-07-23 01:53:30,091 - INFO - joeynmt.training - \tReference:  sau đó , do hậu quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý đã ra lệnh đóng cửa tất cả các rạp chiếu phim , trong vòng một tháng .\n","2021-07-23 01:53:30,091 - INFO - joeynmt.training - \tHypothesis: sau đó , kết quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý lệnh tất cả các rạp chiếu phim sẽ bị đóng cửa , tăng lên một tháng .\n","2021-07-23 01:53:30,091 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step     8000: bleu:  29.66, loss: 84271.9766, ppl:   8.6672, duration: 104.4393s\n","2021-07-23 01:53:51,812 - INFO - joeynmt.training - Epoch  26, Step:     8200, Batch Loss:     2.387865, Tokens per Sec:    22563, Lr: 0.000300\n","2021-07-23 01:54:02,657 - INFO - joeynmt.training - Epoch  26: total training loss 643.27\n","2021-07-23 01:54:02,657 - INFO - joeynmt.training - EPOCH 27\n","2021-07-23 01:54:13,403 - INFO - joeynmt.training - Epoch  27, Step:     8400, Batch Loss:     2.449277, Tokens per Sec:    22506, Lr: 0.000300\n","2021-07-23 01:54:35,044 - INFO - joeynmt.training - Epoch  27, Step:     8600, Batch Loss:     1.822897, Tokens per Sec:    22456, Lr: 0.000300\n","2021-07-23 01:54:36,997 - INFO - joeynmt.training - Epoch  27: total training loss 630.21\n","2021-07-23 01:54:36,997 - INFO - joeynmt.training - EPOCH 28\n","2021-07-23 01:54:56,764 - INFO - joeynmt.training - Epoch  28, Step:     8800, Batch Loss:     2.285385, Tokens per Sec:    22466, Lr: 0.000300\n","2021-07-23 01:55:11,446 - INFO - joeynmt.training - Epoch  28: total training loss 623.67\n","2021-07-23 01:55:11,447 - INFO - joeynmt.training - EPOCH 29\n","2021-07-23 01:55:18,481 - INFO - joeynmt.training - Epoch  29, Step:     9000, Batch Loss:     1.863191, Tokens per Sec:    22569, Lr: 0.000300\n","2021-07-23 01:55:40,107 - INFO - joeynmt.training - Epoch  29, Step:     9200, Batch Loss:     1.873460, Tokens per Sec:    22417, Lr: 0.000300\n","2021-07-23 01:55:45,828 - INFO - joeynmt.training - Epoch  29: total training loss 619.75\n","2021-07-23 01:55:45,828 - INFO - joeynmt.training - EPOCH 30\n","2021-07-23 01:56:01,521 - INFO - joeynmt.training - Epoch  30, Step:     9400, Batch Loss:     2.114608, Tokens per Sec:    21989, Lr: 0.000300\n","2021-07-23 01:56:20,413 - INFO - joeynmt.training - Epoch  30: total training loss 617.68\n","2021-07-23 01:56:20,413 - INFO - joeynmt.training - EPOCH 31\n","2021-07-23 01:56:23,288 - INFO - joeynmt.training - Epoch  31, Step:     9600, Batch Loss:     1.918769, Tokens per Sec:    22102, Lr: 0.000300\n","2021-07-23 01:56:44,675 - INFO - joeynmt.training - Epoch  31, Step:     9800, Batch Loss:     1.778571, Tokens per Sec:    22617, Lr: 0.000300\n","2021-07-23 01:56:54,757 - INFO - joeynmt.training - Epoch  31: total training loss 614.34\n","2021-07-23 01:56:54,757 - INFO - joeynmt.training - EPOCH 32\n","2021-07-23 01:57:06,175 - INFO - joeynmt.training - Epoch  32, Step:    10000, Batch Loss:     1.798112, Tokens per Sec:    22319, Lr: 0.000300\n","2021-07-23 01:57:50,456 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 01:57:50,457 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 01:57:50,457 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 01:57:51,912 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n","2021-07-23 01:57:51,912 - INFO - joeynmt.training - Saving new checkpoint.\n","2021-07-23 01:57:52,300 - INFO - joeynmt.training - Example #0\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - \tSource:     13 cases consisted of one individual each from thailand , china , morocco and india , two individuals each from saudi arabia , ethiopia , iran and three individuals from the uae were detected through early reporting by individuals .\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - \tReference:  13 trường hợp bao gồm một cá nhân mỗi người từ thái lan , trung quốc , morocco và ấn độ , hai cá nhân từ ả rập saudi , ethiopia , iran và ba cá nhân từ uae đã được phát hiện thông qua báo cáo sớm của các cá nhân .\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - \tHypothesis: 13 trường hợp được xác nhận là một cá nhân từ thái lan , trung quốc , trung quốc , hai cá nhân từ ả rập xê , iran và ba cá nhân từ uae được phát hiện đầu bởi các cá nhân .\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - Example #1\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - \tSource:     therefore , koreans have fermented vegetables as food reserves for the winter .\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - \tReference:  do đó , người hàn quốc đã muối rau trái làm thức ăn dự trữ cho mùa đông .\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - \tHypothesis: do đó , người hàn quốc đã phân loại rau như thực phẩm để phục vụ mùa đông .\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - Example #2\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - \tSource:     last season , an incensed fan phoned nottinghamshire police reporting nani 's red card in the champions league match against real madrid as a 'crime ' .\n","2021-07-23 01:57:52,301 - INFO - joeynmt.training - \tReference:  \"mùa trước , một cổ động viên giận dữ gọi điện đến cảnh sát nottinghamshire thông báo rằng chiếc thẻ đỏ của nani trong trận thuộc giải champions league với real madrid là một \" \"tội ác \" \" . \"\n","2021-07-23 01:57:52,302 - INFO - joeynmt.training - \tHypothesis: mùa cuối tuần trước , một incenfan đã đề cập đến cảnh sát cảnh sát của nani card ở champions league gue đối với thực sự bất kỳ đối với thực tế madrid là một 'tội phạm .\n","2021-07-23 01:57:52,302 - INFO - joeynmt.training - Example #3\n","2021-07-23 01:57:52,302 - INFO - joeynmt.training - \tSource:     later , as a result of the pandemic in italy , on march 8 , 2020 the italian government ordered all cinemas to be closed , for up to a month .\n","2021-07-23 01:57:52,302 - INFO - joeynmt.training - \tReference:  sau đó , do hậu quả của đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý đã ra lệnh đóng cửa tất cả các rạp chiếu phim , trong vòng một tháng .\n","2021-07-23 01:57:52,302 - INFO - joeynmt.training - \tHypothesis: sau đó , do đại dịch ở ý , vào ngày 8 tháng 3 năm 2020 , chính phủ ý lệnh tất cả rạp chiếu phim sẽ bị đóng cửa , tăng lên một tháng .\n","2021-07-23 01:57:52,302 - INFO - joeynmt.training - Validation result (greedy) at epoch  32, step    10000: bleu:  33.09, loss: 81172.1250, ppl:   8.0053, duration: 46.1267s\n","2021-07-23 01:58:13,877 - INFO - joeynmt.training - Epoch  32, Step:    10200, Batch Loss:     1.838067, Tokens per Sec:    22619, Lr: 0.000300\n","2021-07-23 01:58:15,276 - INFO - joeynmt.training - Epoch  32: total training loss 605.03\n","2021-07-23 01:58:15,276 - INFO - joeynmt.training - Training ended after  32 epochs.\n","2021-07-23 01:58:15,276 - INFO - joeynmt.training - Best validation result (greedy) at step    10000:   8.01 ppl.\n","2021-07-23 01:58:15,295 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n","2021-07-23 01:58:15,424 - INFO - joeynmt.model - Building an encoder-decoder model...\n","2021-07-23 01:58:15,667 - INFO - joeynmt.model - Enc-dec model built.\n","2021-07-23 01:58:15,723 - INFO - joeynmt.prediction - Decoding on dev set (/content/data/en-vi//dev.4000.bpe.vi)...\n","2021-07-23 01:58:55,249 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 01:58:55,250 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 01:58:55,250 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 01:58:56,636 - INFO - joeynmt.prediction -  dev bleu[intl]:  33.81 [Beam search decoding with beam size = 5 and alpha = 1.0]\n","2021-07-23 01:58:56,637 - INFO - joeynmt.prediction - Translations saved to: models/en_vi_bpe4000_transformer/00010000.hyps.dev\n","2021-07-23 01:58:56,638 - INFO - joeynmt.prediction - Decoding on test set (/content/data/en-vi//test.4000.bpe.vi)...\n","2021-07-23 01:59:42,313 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 01:59:42,313 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 01:59:42,313 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 01:59:44,044 - INFO - joeynmt.prediction - test bleu[intl]:  32.53 [Beam search decoding with beam size = 5 and alpha = 1.0]\n","2021-07-23 01:59:44,046 - INFO - joeynmt.prediction - Translations saved to: models/en_vi_bpe4000_transformer/00010000.hyps.test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RZX1lkRhclY0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627005758202,"user_tz":-420,"elapsed":93024,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"99617093-0a85-4ab5-8712-5e9c8e5d3af1"},"source":["!python -m joeynmt test transformer_en_vi_bpe4000.yaml"],"execution_count":23,"outputs":[{"output_type":"stream","text":["2021-07-23 02:02:28,347 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n","2021-07-23 02:02:28,348 - INFO - joeynmt.data - Building vocabulary...\n","2021-07-23 02:02:28,663 - INFO - joeynmt.data - Loading dev data...\n","2021-07-23 02:02:28,679 - INFO - joeynmt.data - Loading test data...\n","2021-07-23 02:02:28,699 - INFO - joeynmt.data - Data loaded.\n","2021-07-23 02:02:28,736 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n","2021-07-23 02:02:32,364 - INFO - joeynmt.model - Building an encoder-decoder model...\n","2021-07-23 02:02:32,594 - INFO - joeynmt.model - Enc-dec model built.\n","2021-07-23 02:02:32,650 - INFO - joeynmt.prediction - Decoding on dev set (/content/data/en-vi//dev.4000.bpe.vi)...\n","2021-07-23 02:03:09,987 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 02:03:09,988 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 02:03:09,988 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 02:03:11,481 - INFO - joeynmt.prediction -  dev bleu[intl]:  33.81 [Beam search decoding with beam size = 5 and alpha = 1.0]\n","2021-07-23 02:03:11,482 - INFO - joeynmt.prediction - Decoding on test set (/content/data/en-vi//test.4000.bpe.vi)...\n","2021-07-23 02:03:57,576 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 02:03:57,576 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 02:03:57,576 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 02:03:59,272 - INFO - joeynmt.prediction - test bleu[intl]:  32.53 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4wLEmL7ex3v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627005863273,"user_tz":-420,"elapsed":95526,"user":{"displayName":"Dần Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_XlBk6H3Bz1EtbaLB-87Ejle_bXdn8qSa2Wn4IA=s64","userId":"01872814902745695689"}},"outputId":"7e3a348d-489f-4825-bf14-1242befe9c38"},"source":["!python -m joeynmt test transformer_en_vi_bpe4000.yaml --output_path predictions"],"execution_count":24,"outputs":[{"output_type":"stream","text":["2021-07-23 02:04:10,835 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n","2021-07-23 02:04:10,835 - INFO - joeynmt.data - Building vocabulary...\n","2021-07-23 02:04:11,134 - INFO - joeynmt.data - Loading dev data...\n","2021-07-23 02:04:11,150 - INFO - joeynmt.data - Loading test data...\n","2021-07-23 02:04:11,169 - INFO - joeynmt.data - Data loaded.\n","2021-07-23 02:04:11,199 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n","2021-07-23 02:04:14,760 - INFO - joeynmt.model - Building an encoder-decoder model...\n","2021-07-23 02:04:14,998 - INFO - joeynmt.model - Enc-dec model built.\n","2021-07-23 02:04:15,057 - INFO - joeynmt.prediction - Decoding on dev set (/content/data/en-vi//dev.4000.bpe.vi)...\n","2021-07-23 02:04:55,631 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 02:04:55,631 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 02:04:55,631 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 02:04:56,992 - INFO - joeynmt.prediction -  dev bleu[intl]:  33.81 [Beam search decoding with beam size = 5 and alpha = 1.0]\n","2021-07-23 02:04:56,994 - INFO - joeynmt.prediction - Translations saved to: predictions.dev\n","2021-07-23 02:04:56,994 - INFO - joeynmt.prediction - Decoding on test set (/content/data/en-vi//test.4000.bpe.vi)...\n","2021-07-23 02:05:42,591 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","2021-07-23 02:05:42,591 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","2021-07-23 02:05:42,591 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n","2021-07-23 02:05:44,396 - INFO - joeynmt.prediction - test bleu[intl]:  32.53 [Beam search decoding with beam size = 5 and alpha = 1.0]\n","2021-07-23 02:05:44,398 - INFO - joeynmt.prediction - Translations saved to: predictions.test\n"],"name":"stdout"}]}]}